{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industrialización del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Primero se definen las dependencias que son necesarias en cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_packages = ['pandas', 'sklearn', 'mlflow', 'codecarbon', 'numpy', 'lightgbm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "El pipeline se divide en 3 pasos\n",
    "1. Preprocesamiento de datos\n",
    "2. Entrenamiento\n",
    "3. Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path : comp.InputPath() , train_output_csv: comp.OutputPath()):\n",
    "    from src.data_process import DataStorage, FeaturesGenerator\n",
    "  \n",
    "    # Se leen y se preprocesan los datos\n",
    "    data_storage = DataStorage(file_path)\n",
    "    features_generator = FeaturesGenerator(data_storage=data_storage)\n",
    "    train_data = features_generator.generate_features(data_storage.df_data)\n",
    "    train_data.to_csv(train_output_csv)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_preprocess_data_op = func_to_container_op(load_and_preprocess_data,packages_to_install = import_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(train_path: comp.InputPath(),  model_id : comp.OutputPath('GBDTModel')):\n",
    "    import mlflow\n",
    "    import pandas as pd\n",
    "    from codecarbon import EmissionsTracker\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "    # Se definen parametros de preprocesado\n",
    "    params = {\n",
    "        \"learning_rate\" : .1,\n",
    "        \"max_depth\" : 10,\n",
    "        \"n_estimators\" : 500,\n",
    "        \"num_leaves\" : 31\n",
    "    }\n",
    "\n",
    "    # Se inicializa el tracker de emisiones\n",
    "    tracker = EmissionsTracker()\n",
    "    # Se define el modelo en cuestión con los parámetros\n",
    "    my_model = lgb.LGBMRegressor(**params)\n",
    "    # Se inicializa el tracker y se entrena\n",
    "    tracker.start()\n",
    "    my_model.fit(train_data.drop(columns=[\"target\"]), train_data[\"target\"])\n",
    "    emissions = tracker.stop()\n",
    "\n",
    "\n",
    "    # Nos conectamos a nuestro servidor de MLFlow y se crea un nuevo experimento\n",
    "    mlflow.set_tracking_uri(uri=MLFLOW_SERVER_URL)\n",
    "    mlflow.set_experiment(\"Enefit-GBDT\")\n",
    "\n",
    "    # Se registra el experimento en MLFlow\n",
    "    with mlflow.start_run():\n",
    "        # Se juntan los parametros de preprocesado con los de entrenamiento\n",
    "        mlflow.log_params(params)\n",
    "        # Se guardan también las emisiones\n",
    "        mlflow.log_metric(\"emissions\", emissions)\n",
    "        mlflow.set_tag(\"GBDT experiment\", \"First experiment\")\n",
    "        model_info = mlflow.lightgbm.log_model(\n",
    "            lgb_model=my_model,\n",
    "            artifact_path=\"enefit_model\",\n",
    "            input_example=train_data.drop(columns=[\"target\"]),\n",
    "            registered_model_name=\"enefit-lgbt-experiment\"\n",
    "        )\n",
    "\n",
    "    current_experiment=dict(mlflow.get_experiment_by_name(\"Enefit-GBDT\"))\n",
    "    model_id = current_experiment['experiment_id']\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_op = func_to_container_op(train_data, packages_to_install= import_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_data(train_path: comp.InputPath(), model_id: comp.InputPath()):\n",
    "\n",
    "    import mlflow\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    train_data = pd.read_csv(train_path)\n",
    "\n",
    "\n",
    "    X, y = train_data.drop(columns=[\"target\"]), train_data[\"target\"]\n",
    "\n",
    "    # Se crea un Split para series temporales\n",
    "    tsvc = TimeSeriesSplit(n_splits=6)\n",
    "    # Se definen los parametros\n",
    "    params = {\n",
    "        \"learning_rate\" : .1,\n",
    "        \"max_depth\" : 10,\n",
    "        \"n_estimators\" : 500,\n",
    "        \"num_leaves\" : 31\n",
    "    }\n",
    "    # Se ejecuta la validación cruzada para series temporales\n",
    "    scores = cross_val_score(lgb.LGBMRegressor(**params), X,y,cv=tsvc, scoring=\"neg_mean_absolute_error\")\n",
    "\n",
    "    # Se hace la media de las metricas y se multiplica por -1 porque la librería tiene implementada la metrica en negativo: neg_mean_absolute_error\n",
    "    mean_score = np.mean(scores)*-1\n",
    "    with mlflow.start_run(run_id=model_id) as run:\n",
    "        mlflow.log_metric(\"mean_absolute_error\", mean_score)\n",
    "    print(f\"Mean_score: {mean_score}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_op = func_to_container_op(eval_data, packages_to_install= import_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  Se implementa el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_pipeline(url):\n",
    "    load_and_preprocess_data_task = load_and_preprocess_data_op(file = url)   \n",
    "    train_eval_task = train_data_op(train = load_and_preprocess_data_task.outputs['train_output_csv'])\n",
    "    eval_data_task = eval_data_op(train = load_and_preprocess_data_task.outputs['train_output_csv'],model = train_eval_task.output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = kfp.Client() # change arguments accordingly\n",
    "\n",
    "# Running the pipeline\n",
    "client.create_run_from_pipeline_func(\n",
    "    prod_pipeline,\n",
    "    arguments={\n",
    "        'url': '\"../../data\"'\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "docker_image": "",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": true,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan/data",
     "name": "data-g2n6k",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    },
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "house-prices-vanilla-workspace-2wscr",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
