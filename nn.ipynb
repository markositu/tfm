{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8626793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from src.data_process import DataStorage, FeaturesGenerator\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import numpy as np\n",
    "from codecarbon import EmissionsTracker\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5908a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se leen y se preprocesan los datos\n",
    "data_storage = DataStorage()\n",
    "features_generator = FeaturesGenerator(data_storage=data_storage)\n",
    "train_data = features_generator.generate_features(data_storage.df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb89c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen parametros de preprocesado\n",
    "preprocessing_params = {\n",
    "    \"n_features\": 60,\n",
    "    \"is_holiday\": True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cb7fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top correlated features with target\n",
      "['target_168h', 'target_336h', 'target_144h', 'target_mean', 'target_192h', 'target_312h', 'target_48h', 'target_120h', 'target_72h', 'target_216h', 'target_96h', 'target_288h', 'target_240h', 'target_264h', 'target_std', 'installed_capacity', 'target_all_type_sum_168h', 'target_all_type_sum_336h', 'target_all_type_sum_48h', 'target_all_type_sum_72h', 'target_all_county_type_sum_168h', 'target_all_county_type_sum_336h', 'target_all_county_type_sum_48h', 'target_all_county_type_sum_72h', 'eic_count', 'is_consumption', 'product_type', 'is_business', 'county', 'segment', 'cos(hour)', 'surface_solar_radiation_downwards_forecast_local_0h', 'surface_solar_radiation_downwards', 'shortwave_radiation_historical_local_48h', 'shortwave_radiation', 'diffuse_radiation', 'shortwave_radiation_historical_local_168h', 'shortwave_radiation_historical_168h', 'surface_solar_radiation_downwards_forecast_168h', 'diffuse_radiation_historical_168h', 'surface_solar_radiation_downwards_forecast_local_168h', 'direct_solar_radiation_forecast_local_0h', 'direct_solar_radiation', 'direct_solar_radiation_historical_48h', 'diffuse_radiation_historical_local_48h', 'direct_solar_radiation_historical_local_48h', 'diffuse_radiation_historical_local_168h', 'direct_solar_radiation_historical_168h', 'direct_solar_radiation_historical_local_168h', 'direct_solar_radiation_forecast_168h', 'shortwave_radiation_historical_24h', 'direct_solar_radiation_forecast_local_168h', 'diffuse_radiation_historical_24h', 'direct_solar_radiation_historical_24h', 'year', 'dewpoint_historical_24h', 'temperature_historical_24h', 'weekday', 'windspeed_10m_historical_local_48h']\n"
     ]
    }
   ],
   "source": [
    "# Se crea la matriz de correlación\n",
    "correlation_matrix = train_data.corr()\n",
    "\n",
    "# Se ordenan las columnas en función del valor absoluto de la correlación.\n",
    "target_column = 'target'\n",
    "correlation_with_target = correlation_matrix[target_column].abs().sort_values(ascending=False)\n",
    "\n",
    "# Se seleccionan las N features más correlacionadas sin incluir el target\n",
    "top_n_features = correlation_with_target[1:preprocessing_params[\"n_features\"]]  \n",
    "\n",
    "print(\"Top correlated features with\", target_column)\n",
    "print(top_n_features.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3086571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se combiana la columna objetivo con las columnas más correlacionadas en un nuevo DataFrame.\n",
    "selected_features_df = train_data[top_n_features.index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934e3044",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VarianceScaling\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregularizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l2\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2507fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"activation_func\": \"relu\",\n",
    "    \"epochs\": 15,\n",
    "    \"n_hidden_layers\": 3,\n",
    "    \"n_per_layer\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1dcc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = selected_features_df\n",
    "y = train_data[target_column]\n",
    "def create_network (params=params):\n",
    "    model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(128, input_shape=(len(X),), activation='relu'),\n",
    "                tf.keras.layers.Dense(params[\"n_per_layer\"], activation='relu'),\n",
    "                tf.keras.layers.Dense(params[\"n_per_layer\"], activation='relu'),\n",
    "                tf.keras.layers.Dense(params[\"n_per_layer\"], activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "            \n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "my_model = KerasClassifier(build_fn=create_network, epochs=params[\"epochs\"], batch_size=32,\n",
    "                                     verbose=0)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6253c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un Split para series temporales\n",
    "tsvc = TimeSeriesSplit(n_splits=6)\n",
    "# Se definen los parametros\n",
    "params = {\n",
    "    \"learning_rate\" : .1,\n",
    "    \"max_depth\" : 10,\n",
    "    \"n_estimators\" : 500,\n",
    "    \"num_leaves\" : 31\n",
    "}\n",
    "# Se ejecuta la validación cruzada para series temporales\n",
    "scores = cross_val_score(my_model, X,y,cv=tsvc, scoring=\"neg_mean_absolute_error\")\n",
    "\n",
    "# Se hace la media de las metricas y se multiplica por -1 porque la librería tiene implementada la metrica en negativo: neg_mean_absolute_error\n",
    "mean_score = np.mean(scores)*-1\n",
    "print(f\"Mean_score: {mean_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se inicializa el tracker de emisiones\n",
    "tracker = EmissionsTracker()\n",
    "\n",
    "# Se inicializa el tracker y se entrena\n",
    "tracker.start()\n",
    "my_model.fit(X, y)\n",
    "emissions = tracker.stop()\n",
    "print(f\"Emissions:{emissions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan nuevos datos y se prerprocesan\n",
    "data_storage.update_with_new_data(\n",
    "        df_new_client=pd.read_csv(\"data/example_test_files/client.csv\"),\n",
    "        df_new_gas_prices=pd.read_csv(\"data/example_test_files/gas_prices.csv\"),\n",
    "        df_new_electricity_prices=pd.read_csv(\"data/example_test_files/electricity_prices.csv\", parse_dates=[\"forecast_date\",\"origin_date\"]),\n",
    "        df_new_forecast_weather=pd.read_csv(\"data/example_test_files/forecast_weather.csv\", parse_dates=[\"origin_datetime\", \"forecast_datetime\"]),\n",
    "        df_new_historical_weather=pd.read_csv(\"data/example_test_files/historical_weather.csv\", parse_dates=[\"datetime\"]),\n",
    "        df_new_target=pd.read_csv(\"data/example_test_files/revealed_targets.csv\", parse_dates=[\"datetime\"])\n",
    "    )\n",
    "df_test = data_storage.preprocess_test(pd.read_csv(\"data/example_test_files/test.csv\",  parse_dates=[\"prediction_datetime\"]))\n",
    "df_test_features = features_generator.generate_features(df_test, has_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44deaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una predicción\n",
    "predictions = my_model.predict(df_test_features[top_n_features.index.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304fbebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Nos conectamos a nuestro servidor de MLFlow y se crea un nuevo experimento\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_SERVER_URL)\n",
    "mlflow.set_experiment(\"Enefit-Nn\")\n",
    "\n",
    "# MLFlow en este momento no acepta la variables categóricas asi que se convierten estas variales a strings\n",
    "# para que se registren todas las features.\n",
    "X[[\"is_consumption\",\"product_type\",\"is_business\",\"county\",\"segment\"]] = X[[\"is_consumption\",\"product_type\",\"is_business\",\"county\",\"segment\"]].astype(str)\n",
    "signature = mlflow.models.infer_signature(X, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se registra el experimento en MLFlow\n",
    "with mlflow.start_run():\n",
    "    # Se juntan los parametros de preprocesado con los de entrenamiento\n",
    "    mlflow.log_params(params | preprocessing_params)\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mean_score)\n",
    "    # Se guardan también las emisiones\n",
    "    mlflow.log_metric(\"emissions\", emissions)\n",
    "    mlflow.set_tag(\"NN experiment\", \"First experiment\")\n",
    "    model_info = mlflow.keras.log_model(\n",
    "        keras_model=my_model,\n",
    "        artifact_path=\"enefit_model\",\n",
    "        signature=signature,\n",
    "        input_example=X,\n",
    "        registered_model_name=\"enefit-nn-experiment\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
